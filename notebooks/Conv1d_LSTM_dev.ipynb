{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ngr/gdrive/wearables/data/processed/MOD_1000_Woman_Activity_Data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1455-32-NA', '2312-7-1', '2124-8-1'], dtype='<U15')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(list(data.keys()), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out, hidden = nn.LSTM(1, 64)(torch.FloatTensor(np.array([float(i) for i in data['1455-32-NA'][1]])).view(len(data['1455-32-NA'][1]), 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20219, 1, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(64, 2)(lstm_out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0002]],\n",
       "\n",
       "        [[-0.0007]],\n",
       "\n",
       "        [[-0.0007]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0027]],\n",
       "\n",
       "        [[ 0.0007]],\n",
       "\n",
       "        [[ 0.0061]]], grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.LayerNorm(1)(torch.FloatTensor(np.array([float(i) for i in data['1455-32-NA'][1]])).view(-1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BieberLSTM(nn.Module):\n",
    "    def __init__(self, nb_layers, nb_lstm_units=100, embedding_dim=3, batch_size=3):\n",
    "        self.vocab = {'<PAD>': 0, 'is': 1, 'it': 2, 'too': 3, 'late': 4, 'now': 5, 'say': 6, 'sorry': 7, 'ooh': 8,\n",
    "                      'yeah': 9}\n",
    "        self.tags = {'<PAD>': 0, 'VB': 1, 'PRP': 2, 'RB': 3, 'JJ': 4, 'NNP': 5}\n",
    "\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_lstm_units = nb_lstm_units\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # don't count the padding tag for the classifier output\n",
    "        self.nb_tags = len(self.tags) - 1\n",
    "\n",
    "        # when the model is bidirectional we double the output dimension\n",
    "        self.lstm\n",
    "\n",
    "        # build actual NN\n",
    "        self.__build_model()\n",
    "\n",
    "    def __build_model(self):\n",
    "        # build embedding layer first\n",
    "        nb_vocab_words = len(self.vocab)\n",
    "\n",
    "        # whenever the embedding sees the padding index it'll make the whole vector zeros\n",
    "        padding_idx = self.vocab['<PAD>']\n",
    "        self.word_embedding = nn.Embedding(\n",
    "            num_embeddings=nb_vocab_words,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "\n",
    "        # design LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.nb_lstm_units,\n",
    "            num_layers=self.nb_lstm_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # output layer which projects back to tag space\n",
    "        self.hidden_to_tag = nn.Linear(self.nb_lstm_units, self.nb_tags)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_a = torch.randn(self.hparams.nb_lstm_layers, self.batch_size, self.nb_lstm_units)\n",
    "        hidden_b = torch.randn(self.hparams.nb_lstm_layers, self.batch_size, self.nb_lstm_units)\n",
    "\n",
    "        if self.hparams.on_gpu:\n",
    "            hidden_a = hidden_a.cuda()\n",
    "            hidden_b = hidden_b.cuda()\n",
    "\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)\n",
    "\n",
    "    def forward(self, X, X_lengths):\n",
    "        # reset the LSTM hidden state. Must be done before you run a new batch. Otherwise the LSTM will treat\n",
    "        # a new batch as a continuation of a sequence\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "\n",
    "        # ---------------------\n",
    "        # 1. embed the input\n",
    "        # Dim transformation: (batch_size, seq_len, 1) -> (batch_size, seq_len, embedding_dim)\n",
    "        X = self.word_embedding(X)\n",
    "\n",
    "        # ---------------------\n",
    "        # 2. Run through RNN\n",
    "        # TRICK 2 ********************************\n",
    "        # Dim transformation: (batch_size, seq_len, embedding_dim) -> (batch_size, seq_len, nb_lstm_units)\n",
    "\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        X = torch.nn.utils.rnn.pack_padded_sequence(x, X_lengths, batch_first=True)\n",
    "\n",
    "        # now run through LSTM\n",
    "        X, self.hidden = self.lstm(X, self.hidden)\n",
    "\n",
    "        # undo the packing operation\n",
    "        X, _ = torch.nn.utils.rnn.pad_packed_sequence(X, batch_first=True)\n",
    "\n",
    "        # ---------------------\n",
    "        # 3. Project to tag space\n",
    "        # Dim transformation: (batch_size, seq_len, nb_lstm_units) -> (batch_size * seq_len, nb_lstm_units)\n",
    "\n",
    "        # this one is a bit tricky as well. First we need to reshape the data so it goes into the linear layer\n",
    "        X = X.contiguous()\n",
    "        X = X.view(-1, X.shape[2])\n",
    "\n",
    "        # run through actual linear layer\n",
    "        X = self.hidden_to_tag(X)\n",
    "\n",
    "        # ---------------------\n",
    "        # 4. Create softmax activations bc we're doing classification\n",
    "        # Dim transformation: (batch_size * seq_len, nb_lstm_units) -> (batch_size, seq_len, nb_tags)\n",
    "        X = F.log_softmax(X, dim=1)\n",
    "\n",
    "        # I like to reshape for mental sanity so we're back to (batch_size, seq_len, nb_tags)\n",
    "        X = X.view(batch_size, seq_len, self.nb_tags)\n",
    "\n",
    "        Y_hat = X\n",
    "        return Y_hat\n",
    "\n",
    "    def loss(self, Y_hat, Y, X_lengths):\n",
    "        # TRICK 3 ********************************\n",
    "        # before we calculate the negative log likelihood, we need to mask out the activations\n",
    "        # this means we don't want to take into account padded items in the output vector\n",
    "        # simplest way to think about this is to flatten ALL sequences into a REALLY long sequence\n",
    "        # and calculate the loss on that.\n",
    "\n",
    "        # flatten all the labels\n",
    "        Y = Y.view(-1)\n",
    "\n",
    "        # flatten all predictions\n",
    "        Y_hat = Y_hat.view(-1, self.nb_tags)\n",
    "\n",
    "        # create a mask by filtering out all tokens that ARE NOT the padding token\n",
    "        tag_pad_token = self.tags['<PAD>']\n",
    "        mask = (Y > tag_pad_token).float()\n",
    "\n",
    "        # count how many tokens we have\n",
    "        nb_tokens = int(torch.sum(mask).data[0])\n",
    "\n",
    "        # pick the values for the label and zero out the rest with the mask\n",
    "        Y_hat = Y_hat[range(Y_hat.shape[0]), Y] * mask\n",
    "\n",
    "        # compute cross entropy loss which ignores all <PAD> tokens\n",
    "        ce_loss = -torch.sum(Y_hat) / nb_tokens\n",
    "\n",
    "        return ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude sequences fewer than 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences min_n_t: 14\tmax_n_t: 131072\n"
     ]
    }
   ],
   "source": [
    "# return shortest sequence\n",
    "seq_lengths = {}\n",
    "for k, v in data.items():\n",
    "    seq_lengths[k] = len(v[1])\n",
    "print('Sequences min_n_t: {}\\tmax_n_t: {}'.format(min(list(seq_lengths.values())), max(list(seq_lengths.values()))))\n",
    "\n",
    "# return pid of min and delete from data\n",
    "exclude_pids = []\n",
    "for k, v in seq_lengths.items():\n",
    "    if v < 1000:\n",
    "        exclude_pids.append(k)\n",
    "\n",
    "for i in exclude_pids:\n",
    "    del data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences min_n_t: 1262\tmax_n_t: 131072\n"
     ]
    }
   ],
   "source": [
    "# recheck shortest\n",
    "seq_lengths = {}\n",
    "for k, v in data.items():\n",
    "    seq_lengths[k] = len(v[1])\n",
    "print('Sequences min_n_t: {}\\tmax_n_t: {}'.format(min(list(seq_lengths.values())), max(list(seq_lengths.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actigraphydata(torch.utils.data.Dataset):\n",
    "    def __init__(self, datapkl, max_seq_length):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.datapkl = datapkl\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.datapkl.keys())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pid = list(self.datapkl.keys())[idx]\n",
    "        GA = int(pid.split('-')[1])\n",
    "        t, xt = transform(self.datapkl[pid], max_length=self.max_seq_length)\n",
    "        sample = {'x':xt, 'y':torch.tensor(GA)}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = actigraphydata(data, 1200)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(input_seq, max_length=1200):\n",
    "    tod = input_seq[0][:max_length] # time of day\n",
    "    activity = torch.FloatTensor(np.array([float(i) for i in input_seq[1]])[:max_length]).view(-1, 1)\n",
    "    \n",
    "    # log-pseudocount\n",
    "    activity = activity + 1\n",
    "    activity = activity.log()\n",
    "    \n",
    "    # alignment\n",
    "    return (tod, activity)\n",
    "\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, hidden_layer_size, max_seq_length, output_size):\n",
    "        super().__init__()\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        \n",
    "        self.LN = nn.LayerNorm(1)\n",
    "        self.conv1d = nn.Conv1d(1, hidden_layer_size, 5, 1)\n",
    "        self.lstm = nn.LSTM(hidden_layer_size, hidden_layer_size)\n",
    "        self.fc = nn.Linear(hidden_layer_size, output_size)\n",
    "        \n",
    "        # initialize hidden cell\n",
    "        self.hidden_cell = (torch.zeros(1,self.hidden_layer_size,self.hidden_layer_size), \n",
    "                            torch.zeros(1,self.hidden_layer_size,self.hidden_layer_size))\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        xt = self.LN(input_seq)\n",
    "        xt = self.conv1d(xt.view(-1, 1, self.max_seq_length))\n",
    "        lstm_out, self.hidden_cell = self.lstm(xt.view(xt.shape[2], -1, self.hidden_layer_size), self.hidden_cell)\n",
    "        return self.fc(lstm_out.view(-1, self.hidden_layer_size)) # NOTE: calculate 1196 based on conv transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1200, 1])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38272, 2])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EmbeddingNet(32, 1200, 2)(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletSelector:\n",
    "    \"\"\"\n",
    "    Implementation should return indices of anchors, positive and negative samples\n",
    "    return np array of shape [N_triplets x 3]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_triplets(self, embeddings, labels):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class AllTripletSelector(TripletSelector):\n",
    "    \"\"\"\n",
    "    Returns all possible triplets\n",
    "    May be impractical in most cases\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AllTripletSelector, self).__init__()\n",
    "\n",
    "    def get_triplets(self, embeddings, labels):\n",
    "        labels = labels.cpu().data.numpy()\n",
    "        triplets = []\n",
    "        for label in set(labels):\n",
    "            label_mask = (labels == label)\n",
    "            label_indices = np.where(label_mask)[0]\n",
    "            if len(label_indices) < 2:\n",
    "                continue\n",
    "            negative_indices = np.where(np.logical_not(label_mask))[0]\n",
    "            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n",
    "\n",
    "            # Add all negatives for all positive pairs\n",
    "            temp_triplets = [[anchor_positive[0], anchor_positive[1], neg_ind] for anchor_positive in anchor_positives\n",
    "                             for neg_ind in negative_indices]\n",
    "            triplets += temp_triplets\n",
    "\n",
    "        return torch.LongTensor(np.array(triplets))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
